{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09f81352",
   "metadata": {},
   "source": [
    "# Audio Emotion Recognition using RAVDESS\n",
    "    \n",
    "## Project Overview\n",
    "This notebook implements a machine learning system for audio emotion recognition from scratch.\n",
    "We use the RAVDESS dataset and implement the following models manually (without sklearn classifiers):\n",
    "- K-Nearest Neighbors (KNN)\n",
    "- Logistic Regression (Binary & Multi-class OVR)\n",
    "- Soft-Margin SVM (using Gradient Descent)\n",
    "- Polynomial Logistic Regression\n",
    "\n",
    "## Requirements\n",
    "- Python 3.x\n",
    "- NumPy, SciPy, Pandas, Matplotlib, Seaborn\n",
    "- Librosa (for audio processing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953902d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "import librosa.display\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Configuration\n",
    "DATA_PATH = \"Data\"\n",
    "SR = 22050\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40aead61",
   "metadata": {},
   "source": [
    "## 3. Audio Feature Extraction\n",
    "We extract handcrafted features including MFCCs, Chroma, and Spectral features.\n",
    "Total features per sample: 112.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1b636d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file_path, sr=22050):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=sr)\n",
    "        y = librosa.util.normalize(y)\n",
    "        \n",
    "        # MFCCs (40)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
    "        mfcc_mean = np.mean(mfcc, axis=1)\n",
    "        mfcc_std = np.std(mfcc, axis=1)\n",
    "        \n",
    "        # Chroma\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "        chroma_mean = np.mean(chroma, axis=1)\n",
    "        chroma_std = np.std(chroma, axis=1)\n",
    "        \n",
    "        # Spectral Features\n",
    "        cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "        cent_mean = np.mean(cent)\n",
    "        cent_std = np.std(cent)\n",
    "        \n",
    "        rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "        rolloff_mean = np.mean(rolloff)\n",
    "        rolloff_std = np.std(rolloff)\n",
    "        \n",
    "        bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "        bw_mean = np.mean(bw)\n",
    "        bw_std = np.std(bw)\n",
    "        \n",
    "        zcr = librosa.feature.zero_crossing_rate(y)\n",
    "        zcr_mean = np.mean(zcr)\n",
    "        zcr_std = np.std(zcr)\n",
    "        \n",
    "        features = np.concatenate([\n",
    "            mfcc_mean, mfcc_std,\n",
    "            chroma_mean, chroma_std,\n",
    "            [cent_mean, cent_std, rolloff_mean, rolloff_std, bw_mean, bw_std, zcr_mean, zcr_std]\n",
    "        ])\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7af5cf",
   "metadata": {},
   "source": [
    "## 4. Data Loading and Preprocessing\n",
    "We load the RAVDESS dataset, parse filenames for emotion labels, and extract features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483da51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMOTIONS = {\n",
    "    '01': 'neutral', '02': 'calm', '03': 'happy', '04': 'sad',\n",
    "    '05': 'angry', '06': 'fearful', '07': 'disgust', '08': 'surprised'\n",
    "}\n",
    "\n",
    "def load_data(data_path):\n",
    "    X, y = [], []\n",
    "    wav_files = glob.glob(os.path.join(data_path, \"**/*.wav\"), recursive=True)\n",
    "    print(f\"Found {len(wav_files)} files.\")\n",
    "    \n",
    "    for i, file_path in enumerate(wav_files):\n",
    "        filename = os.path.basename(file_path)\n",
    "        parts = filename.split('-')\n",
    "        if len(parts) != 7: continue\n",
    "            \n",
    "        emotion_label = EMOTIONS[parts[2]]\n",
    "        features = extract_features(file_path)\n",
    "        if features is not None:\n",
    "            X.append(features)\n",
    "            y.append(emotion_label)\n",
    "        \n",
    "        if i % 500 == 0: print(f\"Processed {i} files...\")\n",
    "            \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load Data (Uncomment to run - takes time)\n",
    "# X, y = load_data(DATA_PATH)\n",
    "# np.save('X.npy', X)\n",
    "# np.save('y.npy', y)\n",
    "\n",
    "# For demonstration, we assume data is loaded or we load a subset\n",
    "if os.path.exists('X.npy') and os.path.exists('y.npy'):\n",
    "    X = np.load('X.npy')\n",
    "    y = np.load('y.npy')\n",
    "else:\n",
    "    # Fallback if files don't exist (e.g. first run)\n",
    "    X, y = load_data(DATA_PATH)\n",
    "    np.save('X.npy', X)\n",
    "    np.save('y.npy', y)\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f4e1c0",
   "metadata": {},
   "source": [
    "## 5. Models Implemented from Scratch\n",
    "Here we implement KNN, Logistic Regression, and SVM without using sklearn classifiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33c1dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardScalerCustom:\n",
    "    def fit(self, X):\n",
    "        self.mean = np.mean(X, axis=0)\n",
    "        self.std = np.std(X, axis=0)\n",
    "        self.std[self.std == 0] = 1.0\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return (X - self.mean) / self.std\n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "class KNNClassifier:\n",
    "    def __init__(self, k=5):\n",
    "        self.k = k\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = np.array(X)\n",
    "        self.y_train = np.array(y)\n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        preds = []\n",
    "        for x in X:\n",
    "            dists = np.sqrt(np.sum((self.X_train - x)**2, axis=1))\n",
    "            k_idx = np.argsort(dists)[:self.k]\n",
    "            k_labels = self.y_train[k_idx]\n",
    "            unique, counts = np.unique(k_labels, return_counts=True)\n",
    "            preds.append(unique[np.argmax(counts)])\n",
    "        return np.array(preds)\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.01, epochs=1000, reg=0.0):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.reg = reg\n",
    "        self.models = []\n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-np.clip(z, -250, 250)))\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        X = np.array(X)\n",
    "        n_samples, n_features = X.shape\n",
    "        self.models = []\n",
    "        for c in self.classes:\n",
    "            y_bin = np.where(y == c, 1, 0)\n",
    "            w = np.zeros(n_features)\n",
    "            b = 0\n",
    "            for _ in range(self.epochs):\n",
    "                linear = np.dot(X, w) + b\n",
    "                y_pred = self._sigmoid(linear)\n",
    "                dw = (1/n_samples) * np.dot(X.T, (y_pred - y_bin)) + (self.reg * w)\n",
    "                db = (1/n_samples) * np.sum(y_pred - y_bin)\n",
    "                w -= self.lr * dw\n",
    "                b -= self.lr * db\n",
    "            self.models.append((w, b))\n",
    "    def predict(self, X):\n",
    "        probs = []\n",
    "        for w, b in self.models:\n",
    "            probs.append(self._sigmoid(np.dot(X, w) + b))\n",
    "        return self.classes[np.argmax(np.array(probs).T, axis=1)]\n",
    "\n",
    "class SVM:\n",
    "    def __init__(self, lr=0.001, lambda_param=0.01, epochs=1000):\n",
    "        self.lr = lr\n",
    "        self.lambda_param = lambda_param\n",
    "        self.epochs = epochs\n",
    "        self.models = []\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        X = np.array(X)\n",
    "        self.models = []\n",
    "        for c in self.classes:\n",
    "            y_bin = np.where(y == c, 1, -1)\n",
    "            w = np.zeros(X.shape[1])\n",
    "            b = 0\n",
    "            for _ in range(self.epochs):\n",
    "                for idx, x_i in enumerate(X):\n",
    "                    if y_bin[idx] * (np.dot(x_i, w) - b) >= 1:\n",
    "                        w -= self.lr * (2 * self.lambda_param * w)\n",
    "                    else:\n",
    "                        w -= self.lr * (2 * self.lambda_param * w - np.dot(x_i, y_bin[idx]))\n",
    "                        b -= self.lr * y_bin[idx]\n",
    "            self.models.append((w, b))\n",
    "    def predict(self, X):\n",
    "        scores = np.zeros((X.shape[0], len(self.classes)))\n",
    "        for i, (w, b) in enumerate(self.models):\n",
    "            scores[:, i] = np.dot(X, w) - b\n",
    "        return self.classes[np.argmax(scores, axis=1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1a7d87",
   "metadata": {},
   "source": [
    "## 6. Training and Evaluation\n",
    "We split the data, normalize it, and train our custom models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a46cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Normalize\n",
    "scaler = StandardScalerCustom()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train KNN\n",
    "print(\"Training KNN...\")\n",
    "knn = KNNClassifier(k=5)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "y_pred_knn = knn.predict(X_test_scaled)\n",
    "print(\"KNN Accuracy:\", accuracy_score(y_test, y_pred_knn))\n",
    "\n",
    "# Train Logistic Regression\n",
    "print(\"Training Logistic Regression...\")\n",
    "lr = LogisticRegression(lr=0.01, epochs=500, reg=0.01)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "y_pred_lr = lr.predict(X_test_scaled)\n",
    "print(\"LR Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
    "\n",
    "# Train SVM\n",
    "print(\"Training SVM...\")\n",
    "svm = SVM(lr=0.001, epochs=500, lambda_param=0.01)\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "y_pred_svm = svm.predict(X_test_scaled)\n",
    "print(\"SVM Accuracy:\", accuracy_score(y_test, y_pred_svm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01a7358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for Best Model (e.g. SVM)\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(y_test, y_pred_svm, labels=svm.classes)\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=svm.classes, yticklabels=svm.classes, cmap='Blues')\n",
    "plt.title(\"SVM Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff247fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Best Model\n",
    "best_model = svm # Assume SVM is best for now\n",
    "with open('model_parameters.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(\"Model exported.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
