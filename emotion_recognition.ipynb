{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Emotion Recognition using RAVDESS Dataset\n",
    "\n",
    "**Author:** ML Project Documentation  \n",
    "**Date:** 2024  \n",
    "**Dataset:** RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song)\n",
    "\n",
    "---\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "This notebook documents a complete machine learning pipeline for audio emotion recognition. We implement **four classification models from scratch** using only NumPy/SciPy, without using sklearn's built-in classifiers.\n",
    "\n",
    "### Key Highlights\n",
    "- **Dataset:** 2,880 audio files from 24 actors\n",
    "- **Features:** 112 handcrafted audio features (MFCCs, Chroma, Spectral)\n",
    "- **Models:** KNN, Logistic Regression, SVM, Polynomial Logistic Regression\n",
    "- **Best Accuracy:** ~88% (Polynomial Logistic Regression)\n",
    "- **Emotions:** 8 classes (Neutral, Calm, Happy, Sad, Angry, Fearful, Disgust, Surprised)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction & Problem Statement\n",
    "\n",
    "### Objective\n",
    "Build a system that can automatically detect emotions from speech audio using machine learning.\n",
    "\n",
    "### Why This Matters\n",
    "- **Human-Computer Interaction:** Enable more natural interactions\n",
    "- **Mental Health:** Detect emotional states in therapy\n",
    "- **Customer Service:** Analyze customer sentiment in call centers\n",
    "\n",
    "### Approach\n",
    "1. Extract handcrafted audio features using librosa\n",
    "2. Implement ML models from scratch (no sklearn classifiers)\n",
    "3. Compare model performance\n",
    "4. Deploy best model in a web application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6377a63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "import librosa.display\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
    "from IPython.display import Audio\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Configuration\n",
    "DATA_PATH = \"Data\"\n",
    "TEST_DATA_PATH = \"Data Test/Emotions\"\n",
    "SR = 22050  # Sample rate\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(\"✓ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f5a42d",
   "metadata": {},
   "source": [
    "## 2. Dataset Exploration\n",
    "\n",
    "### RAVDESS Dataset Structure\n",
    "\n",
    "The RAVDESS dataset contains 2,880 audio files with the following naming convention:\n",
    "\n",
    "**Filename Format:** `Modality-VocalChannel-Emotion-EmotionalIntensity-Statement-Repetition-Actor.wav`\n",
    "\n",
    "Example: `03-01-06-01-02-01-12.wav`\n",
    "- Modality: 03 (audio-video)\n",
    "- Vocal Channel: 01 (speech)\n",
    "- **Emotion: 06 (fearful)** ← We extract this\n",
    "- Intensity: 01 (normal)\n",
    "- Statement: 02\n",
    "- Repetition: 01\n",
    "- Actor: 12\n",
    "\n",
    "### Emotion Labels\n",
    "- 01: Neutral\n",
    "- 02: Calm\n",
    "- 03: Happy\n",
    "- 04: Sad\n",
    "- 05: Angry\n",
    "- 06: Fearful\n",
    "- 07: Disgust\n",
    "- 08: Surprised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50950fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotion mapping\n",
    "EMOTIONS = {\n",
    "    '01': 'neutral',\n",
    "    '02': 'calm',\n",
    "    '03': 'happy',\n",
    "    '04': 'sad',\n",
    "    '05': 'angry',\n",
    "    '06': 'fearful',\n",
    "    '07': 'disgust',\n",
    "    '08': 'surprised'\n",
    "}\n",
    "\n",
    "# Count files\n",
    "wav_files = glob.glob(os.path.join(DATA_PATH, \"**/*.wav\"), recursive=True)\n",
    "test_wav_files = glob.glob(os.path.join(TEST_DATA_PATH, \"**/*.wav\"), recursive=True)\n",
    "\n",
    "print(f\"Training audio files: {len(wav_files)}\")\n",
    "print(f\"Test audio files: {len(test_wav_files)}\")\n",
    "print(f\"\\nSample training filenames:\")\n",
    "for f in wav_files[:3]:\n",
    "    print(f\"  {os.path.basename(f)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b484546d",
   "metadata": {},
   "source": [
    "## 3. Audio Feature Extraction\n",
    "\n",
    "### Feature Engineering Strategy\n",
    "\n",
    "We extract **112 handcrafted features** from each audio file:\n",
    "\n",
    "#### 3.1 MFCCs (80 features)\n",
    "- **What:** Mel-Frequency Cepstral Coefficients\n",
    "- **Why:** Captures timbral texture, widely used in speech recognition\n",
    "- **How:** 40 coefficients × 2 (mean + std) = 80 features\n",
    "\n",
    "#### 3.2 Chroma Features (24 features)\n",
    "- **What:** Pitch class profiles\n",
    "- **Why:** Represents harmonic and melodic characteristics\n",
    "- **How:** 12 pitch classes × 2 (mean + std) = 24 features\n",
    "\n",
    "#### 3.3 Spectral Features (8 features)\n",
    "- **Spectral Centroid:** Center of mass of spectrum (brightness)\n",
    "- **Spectral Rolloff:** Frequency below which 85% of energy is contained\n",
    "- **Spectral Bandwidth:** Width of the spectrum\n",
    "- **Zero Crossing Rate:** Rate of sign changes in signal\n",
    "- Each: mean + std = 8 features total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7889f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file_path, sr=22050):\n",
    "    \"\"\"\n",
    "    Extract 112 audio features from a WAV file.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Path to audio file\n",
    "    sr : int\n",
    "        Sample rate (default: 22050 Hz)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    features : np.array\n",
    "        112-dimensional feature vector\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load audio\n",
    "        y, sr = librosa.load(file_path, sr=sr)\n",
    "        y = librosa.util.normalize(y)\n",
    "        \n",
    "        # 1. MFCCs (40 coefficients)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
    "        mfcc_mean = np.mean(mfcc, axis=1)\n",
    "        mfcc_std = np.std(mfcc, axis=1)\n",
    "        \n",
    "        # 2. Chroma (12 pitch classes)\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "        chroma_mean = np.mean(chroma, axis=1)\n",
    "        chroma_std = np.std(chroma, axis=1)\n",
    "        \n",
    "        # 3. Spectral Features\n",
    "        cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "        rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "        bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "        zcr = librosa.feature.zero_crossing_rate(y)\n",
    "        \n",
    "        # Aggregate spectral features\n",
    "        spectral_features = [\n",
    "            np.mean(cent), np.std(cent),\n",
    "            np.mean(rolloff), np.std(rolloff),\n",
    "            np.mean(bw), np.std(bw),\n",
    "            np.mean(zcr), np.std(zcr)\n",
    "        ]\n",
    "        \n",
    "        # Concatenate all features\n",
    "        features = np.concatenate([\n",
    "            mfcc_mean, mfcc_std,      # 80 features\n",
    "            chroma_mean, chroma_std,  # 24 features\n",
    "            spectral_features         # 8 features\n",
    "        ])\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test feature extraction\n",
    "if len(wav_files) > 0:\n",
    "    test_features = extract_features(wav_files[0])\n",
    "    print(f\"Feature vector shape: {test_features.shape}\")\n",
    "    print(f\"Total features: {len(test_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab04b9e",
   "metadata": {},
   "source": [
    "### Visualize Sample Audio with Multiple Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ff5fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and visualize multiple samples from different emotions\n",
    "sample_emotions = ['happy', 'sad', 'angry', 'neutral']\n",
    "sample_codes = {'happy': '03', 'sad': '04', 'angry': '05', 'neutral': '01'}\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(16, 14))\n",
    "fig.suptitle('Audio Feature Comparison Across Emotions', fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "for idx, emotion in enumerate(sample_emotions):\n",
    "    # Find a file with this emotion\n",
    "    emotion_code = sample_codes[emotion]\n",
    "    sample_file = None\n",
    "    for f in wav_files:\n",
    "        if f'-{emotion_code}-' in os.path.basename(f):\n",
    "            sample_file = f\n",
    "            break\n",
    "    \n",
    "    if sample_file:\n",
    "        y_sample, sr_sample = librosa.load(sample_file, sr=SR)\n",
    "        \n",
    "        # Waveform\n",
    "        librosa.display.waveshow(y_sample, sr=sr_sample, ax=axes[idx, 0], color='steelblue')\n",
    "        axes[idx, 0].set_title(f'{emotion.capitalize()} - Waveform', fontweight='bold')\n",
    "        axes[idx, 0].set_ylabel('Amplitude')\n",
    "        \n",
    "        # MFCC\n",
    "        mfcc_sample = librosa.feature.mfcc(y=y_sample, sr=sr_sample, n_mfcc=20)\n",
    "        img1 = librosa.display.specshow(mfcc_sample, x_axis='time', ax=axes[idx, 1], cmap='viridis')\n",
    "        axes[idx, 1].set_title(f'{emotion.capitalize()} - MFCC', fontweight='bold')\n",
    "        \n",
    "        # Chroma\n",
    "        chroma_sample = librosa.feature.chroma_stft(y=y_sample, sr=sr_sample)\n",
    "        img2 = librosa.display.specshow(chroma_sample, x_axis='time', y_axis='chroma', ax=axes[idx, 2], cmap='coolwarm')\n",
    "        axes[idx, 2].set_title(f'{emotion.capitalize()} - Chroma', fontweight='bold')\n",
    "        \n",
    "        # Spectral Centroid\n",
    "        cent = librosa.feature.spectral_centroid(y=y_sample, sr=sr_sample)\n",
    "        times = librosa.times_like(cent, sr=sr_sample)\n",
    "        axes[idx, 3].plot(times, cent.T, color='coral', linewidth=1.5)\n",
    "        axes[idx, 3].set_title(f'{emotion.capitalize()} - Spectral Centroid', fontweight='bold')\n",
    "        axes[idx, 3].set_xlabel('Time (s)')\n",
    "        axes[idx, 3].set_ylabel('Hz')\n",
    "        axes[idx, 3].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78b64a6",
   "metadata": {},
   "source": [
    "## 4. Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c2e48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    \"\"\"\n",
    "    Load RAVDESS dataset and extract features.\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    wav_files = glob.glob(os.path.join(data_path, \"**/*.wav\"), recursive=True)\n",
    "    print(f\"Found {len(wav_files)} audio files.\")\n",
    "    \n",
    "    for i, file_path in enumerate(wav_files):\n",
    "        filename = os.path.basename(file_path)\n",
    "        parts = filename.split('-')\n",
    "        \n",
    "        if len(parts) != 7:\n",
    "            continue\n",
    "        \n",
    "        # Extract emotion label from filename\n",
    "        emotion_code = parts[2]\n",
    "        emotion_label = EMOTIONS[emotion_code]\n",
    "        \n",
    "        # Extract features\n",
    "        features = extract_features(file_path)\n",
    "        if features is not None:\n",
    "            X.append(features)\n",
    "            y.append(emotion_label)\n",
    "        \n",
    "        if (i + 1) % 500 == 0:\n",
    "            print(f\"Processed {i + 1}/{len(wav_files)} files...\")\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load or cache data\n",
    "if os.path.exists('X.npy') and os.path.exists('y.npy'):\n",
    "    print(\"Loading cached training data...\")\n",
    "    X = np.load('X.npy')\n",
    "    y = np.load('y.npy')\n",
    "else:\n",
    "    print(\"Extracting features from training audio files...\")\n",
    "    X, y = load_data(DATA_PATH)\n",
    "    np.save('X.npy', X)\n",
    "    np.save('y.npy', y)\n",
    "\n",
    "print(f\"\\n✓ Training dataset loaded!\")\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355e0417",
   "metadata": {},
   "source": [
    "### Dataset Statistics & Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e952bcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution\n",
    "emotion_counts = pd.Series(y).value_counts().sort_index()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Training Dataset Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Bar plot\n",
    "emotion_counts.plot(kind='bar', ax=axes[0, 0], color='steelblue', edgecolor='black')\n",
    "axes[0, 0].set_title('Emotion Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Emotion')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Pie chart\n",
    "colors = plt.cm.Set3(range(len(emotion_counts)))\n",
    "axes[0, 1].pie(emotion_counts, labels=emotion_counts.index, autopct='%1.1f%%', \n",
    "               startangle=90, colors=colors, textprops={'fontsize': 10})\n",
    "axes[0, 1].set_title('Emotion Proportions', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Feature statistics\n",
    "feature_means = np.mean(X, axis=0)\n",
    "axes[1, 0].plot(feature_means, color='coral', linewidth=1.5)\n",
    "axes[1, 0].set_title('Mean Feature Values Across Dataset', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Feature Index')\n",
    "axes[1, 0].set_ylabel('Mean Value')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].axvline(x=80, color='red', linestyle='--', alpha=0.5, label='MFCC|Chroma')\n",
    "axes[1, 0].axvline(x=104, color='blue', linestyle='--', alpha=0.5, label='Chroma|Spectral')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Feature variance\n",
    "feature_stds = np.std(X, axis=0)\n",
    "axes[1, 1].plot(feature_stds, color='purple', linewidth=1.5)\n",
    "axes[1, 1].set_title('Feature Standard Deviation', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Feature Index')\n",
    "axes[1, 1].set_ylabel('Std Dev')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(emotion_counts)\n",
    "print(f\"\\nDataset is {'balanced' if emotion_counts.std() < 50 else 'imbalanced'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b20d0d",
   "metadata": {},
   "source": [
    "## 5. Model Implementation (From Scratch)\n",
    "\n",
    "We implement all models without using sklearn's built-in classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc88842",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardScalerCustom:\n",
    "    \"\"\"Custom implementation of StandardScaler.\"\"\"\n",
    "    def fit(self, X):\n",
    "        self.mean = np.mean(X, axis=0)\n",
    "        self.std = np.std(X, axis=0)\n",
    "        self.std[self.std == 0] = 1.0\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return (X - self.mean) / self.std\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        return self.fit(X).transform(X)\n",
    "\n",
    "class KNNClassifier:\n",
    "    \"\"\"K-Nearest Neighbors from scratch.\"\"\"\n",
    "    def __init__(self, k=5):\n",
    "        self.k = k\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.X_train = np.array(X)\n",
    "        self.y_train = np.array(y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            distances = np.sqrt(np.sum((self.X_train - x)**2, axis=1))\n",
    "            k_indices = np.argsort(distances)[:self.k]\n",
    "            k_labels = self.y_train[k_indices]\n",
    "            unique, counts = np.unique(k_labels, return_counts=True)\n",
    "            predictions.append(unique[np.argmax(counts)])\n",
    "        return np.array(predictions)\n",
    "\n",
    "class LogisticRegression:\n",
    "    \"\"\"Logistic Regression with One-vs-Rest strategy.\"\"\"\n",
    "    def __init__(self, lr=0.01, epochs=1000, reg=0.0):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.reg = reg\n",
    "        self.models = []\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-np.clip(z, -250, 250)))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        X = np.array(X)\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        for c in self.classes:\n",
    "            y_binary = np.where(y == c, 1, 0)\n",
    "            w = np.zeros(n_features)\n",
    "            b = 0\n",
    "            \n",
    "            for _ in range(self.epochs):\n",
    "                linear = np.dot(X, w) + b\n",
    "                y_pred = self._sigmoid(linear)\n",
    "                dw = (1/n_samples) * np.dot(X.T, (y_pred - y_binary)) + (self.reg * w)\n",
    "                db = (1/n_samples) * np.sum(y_pred - y_binary)\n",
    "                w -= self.lr * dw\n",
    "                b -= self.lr * db\n",
    "            \n",
    "            self.models.append((w, b))\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        probs = []\n",
    "        for w, b in self.models:\n",
    "            probs.append(self._sigmoid(np.dot(X, w) + b))\n",
    "        return np.array(probs).T\n",
    "    \n",
    "    def predict(self, X):\n",
    "        probs = self.predict_proba(X)\n",
    "        return self.classes[np.argmax(probs, axis=1)]\n",
    "\n",
    "class SVM:\n",
    "    \"\"\"Soft-Margin SVM with Gradient Descent.\"\"\"\n",
    "    def __init__(self, lr=0.001, lambda_param=0.01, epochs=1000):\n",
    "        self.lr = lr\n",
    "        self.lambda_param = lambda_param\n",
    "        self.epochs = epochs\n",
    "        self.models = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        X = np.array(X)\n",
    "        \n",
    "        for c in self.classes:\n",
    "            y_binary = np.where(y == c, 1, -1)\n",
    "            w = np.zeros(X.shape[1])\n",
    "            b = 0\n",
    "            \n",
    "            for _ in range(self.epochs):\n",
    "                for idx, x_i in enumerate(X):\n",
    "                    if y_binary[idx] * (np.dot(x_i, w) - b) >= 1:\n",
    "                        w -= self.lr * (2 * self.lambda_param * w)\n",
    "                    else:\n",
    "                        w -= self.lr * (2 * self.lambda_param * w - np.dot(x_i, y_binary[idx]))\n",
    "                        b -= self.lr * y_binary[idx]\n",
    "            \n",
    "            self.models.append((w, b))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        scores = np.zeros((X.shape[0], len(self.classes)))\n",
    "        for i, (w, b) in enumerate(self.models):\n",
    "            scores[:, i] = np.dot(X, w) - b\n",
    "        return self.classes[np.argmax(scores, axis=1)]\n",
    "\n",
    "class PolynomialFeatureGenerator:\n",
    "    \"\"\"Generate polynomial features.\"\"\"\n",
    "    def __init__(self, degree=2):\n",
    "        self.degree = degree\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = np.array(X)\n",
    "        n_samples, n_features = X.shape\n",
    "        features = [np.ones(n_samples)]\n",
    "        \n",
    "        for i in range(n_features):\n",
    "            features.append(X[:, i])\n",
    "        \n",
    "        if self.degree >= 2:\n",
    "            for i in range(n_features):\n",
    "                for j in range(i, n_features):\n",
    "                    features.append(X[:, i] * X[:, j])\n",
    "        \n",
    "        return np.stack(features, axis=1)\n",
    "\n",
    "print(\"✓ Models implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c9bc66",
   "metadata": {},
   "source": [
    "## 6. Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d42f709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "# Normalize\n",
    "scaler = StandardScalerCustom()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training set: {X_train_scaled.shape}\")\n",
    "print(f\"Test set: {X_test_scaled.shape}\")\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "predictions = {}\n",
    "\n",
    "# 1. KNN\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Training K-Nearest Neighbors...\")\n",
    "knn = KNNClassifier(k=5)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "y_pred_knn = knn.predict(X_test_scaled)\n",
    "predictions['KNN'] = y_pred_knn\n",
    "results['KNN'] = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred_knn),\n",
    "    'f1': f1_score(y_test, y_pred_knn, average='weighted'),\n",
    "    'precision': precision_score(y_test, y_pred_knn, average='weighted'),\n",
    "    'recall': recall_score(y_test, y_pred_knn, average='weighted')\n",
    "}\n",
    "print(f\"KNN - Acc: {results['KNN']['accuracy']:.4f}, F1: {results['KNN']['f1']:.4f}\")\n",
    "\n",
    "# 2. Logistic Regression\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Training Logistic Regression...\")\n",
    "lr = LogisticRegression(lr=0.01, epochs=500, reg=0.01)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "y_pred_lr = lr.predict(X_test_scaled)\n",
    "predictions['LogisticRegression'] = y_pred_lr\n",
    "results['LogisticRegression'] = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred_lr),\n",
    "    'f1': f1_score(y_test, y_pred_lr, average='weighted'),\n",
    "    'precision': precision_score(y_test, y_pred_lr, average='weighted'),\n",
    "    'recall': recall_score(y_test, y_pred_lr, average='weighted')\n",
    "}\n",
    "print(f\"LR - Acc: {results['LogisticRegression']['accuracy']:.4f}, F1: {results['LogisticRegression']['f1']:.4f}\")\n",
    "\n",
    "# 3. SVM\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Training SVM...\")\n",
    "svm = SVM(lr=0.001, epochs=500, lambda_param=0.01)\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "y_pred_svm = svm.predict(X_test_scaled)\n",
    "predictions['SVM'] = y_pred_svm\n",
    "results['SVM'] = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred_svm),\n",
    "    'f1': f1_score(y_test, y_pred_svm, average='weighted'),\n",
    "    'precision': precision_score(y_test, y_pred_svm, average='weighted'),\n",
    "    'recall': recall_score(y_test, y_pred_svm, average='weighted')\n",
    "}\n",
    "print(f\"SVM - Acc: {results['SVM']['accuracy']:.4f}, F1: {results['SVM']['f1']:.4f}\")\n",
    "\n",
    "# 4. Polynomial Logistic Regression\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Training Polynomial Logistic Regression...\")\n",
    "poly = PolynomialFeatureGenerator(degree=2)\n",
    "X_train_poly = poly.transform(X_train_scaled)\n",
    "X_test_poly = poly.transform(X_test_scaled)\n",
    "lr_poly = LogisticRegression(lr=0.005, epochs=500, reg=0.01)\n",
    "lr_poly.fit(X_train_poly, y_train)\n",
    "y_pred_poly = lr_poly.predict(X_test_poly)\n",
    "predictions['PolynomialLR'] = y_pred_poly\n",
    "results['PolynomialLR'] = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred_poly),\n",
    "    'f1': f1_score(y_test, y_pred_poly, average='weighted'),\n",
    "    'precision': precision_score(y_test, y_pred_poly, average='weighted'),\n",
    "    'recall': recall_score(y_test, y_pred_poly, average='weighted')\n",
    "}\n",
    "print(f\"Poly LR - Acc: {results['PolynomialLR']['accuracy']:.4f}, F1: {results['PolynomialLR']['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deec65b",
   "metadata": {},
   "source": [
    "### Comprehensive Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa2266c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results table\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df = results_df.sort_values('accuracy', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(results_df.to_string())\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Model Performance Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# All metrics comparison\n",
    "results_df.plot(kind='bar', ax=axes[0, 0], color=['steelblue', 'coral', 'lightgreen', 'plum'])\n",
    "axes[0, 0].set_title('All Metrics Comparison', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Score')\n",
    "axes[0, 0].set_xlabel('Model')\n",
    "axes[0, 0].legend(['Accuracy', 'F1', 'Precision', 'Recall'], loc='lower right')\n",
    "axes[0, 0].set_ylim([0, 1])\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Accuracy only\n",
    "results_df['accuracy'].plot(kind='barh', ax=axes[0, 1], color='steelblue', edgecolor='black')\n",
    "axes[0, 1].set_title('Accuracy Comparison', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Accuracy')\n",
    "axes[0, 1].set_xlim([0, 1])\n",
    "axes[0, 1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# F1 Score\n",
    "results_df['f1'].plot(kind='barh', ax=axes[1, 0], color='coral', edgecolor='black')\n",
    "axes[1, 0].set_title('F1-Score Comparison', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('F1-Score')\n",
    "axes[1, 0].set_xlim([0, 1])\n",
    "axes[1, 0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Precision vs Recall\n",
    "axes[1, 1].scatter(results_df['precision'], results_df['recall'], s=200, alpha=0.6, c=range(len(results_df)), cmap='viridis')\n",
    "for idx, model in enumerate(results_df.index):\n",
    "    axes[1, 1].annotate(model, (results_df.loc[model, 'precision'], results_df.loc[model, 'recall']), \n",
    "                       fontsize=9, ha='center')\n",
    "axes[1, 1].set_title('Precision vs Recall', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Precision')\n",
    "axes[1, 1].set_ylabel('Recall')\n",
    "axes[1, 1].set_xlim([0, 1])\n",
    "axes[1, 1].set_ylim([0, 1])\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2dcffc",
   "metadata": {},
   "source": [
    "### Confusion Matrices for All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d52356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices for all models\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "fig.suptitle('Confusion Matrices - All Models', fontsize=16, fontweight='bold')\n",
    "\n",
    "model_list = ['KNN', 'LogisticRegression', 'SVM', 'PolynomialLR']\n",
    "model_objects = [knn, lr, svm, lr_poly]\n",
    "\n",
    "for idx, (model_name, model_obj) in enumerate(zip(model_list, model_objects)):\n",
    "    row, col = idx // 2, idx % 2\n",
    "    y_pred = predictions[model_name]\n",
    "    \n",
    "    # Get classes\n",
    "    if hasattr(model_obj, 'classes'):\n",
    "        classes = model_obj.classes\n",
    "    else:\n",
    "        classes = np.unique(y_test)\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred, labels=classes)\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=classes, yticklabels=classes, \n",
    "                ax=axes[row, col], cbar_kws={'label': 'Count'})\n",
    "    \n",
    "    acc = results[model_name]['accuracy']\n",
    "    axes[row, col].set_title(f'{model_name} (Acc: {acc:.3f})', fontsize=12, fontweight='bold')\n",
    "    axes[row, col].set_xlabel('Predicted')\n",
    "    axes[row, col].set_ylabel('True')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeef0f5e",
   "metadata": {},
   "source": [
    "### Per-Class Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc324e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model per-class analysis\n",
    "best_model_name = results_df.index[0]\n",
    "y_pred_best = predictions[best_model_name]\n",
    "\n",
    "if hasattr(eval(best_model_name.lower().replace('polynomiallr', 'lr_poly')), 'classes'):\n",
    "    classes = eval(best_model_name.lower().replace('polynomiallr', 'lr_poly')).classes\n",
    "else:\n",
    "    classes = np.unique(y_test)\n",
    "\n",
    "# Classification report\n",
    "print(f\"\\nDetailed Classification Report - {best_model_name}\")\n",
    "print(\"=\"*70)\n",
    "print(classification_report(y_test, y_pred_best, target_names=classes))\n",
    "\n",
    "# Per-class metrics visualization\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision, recall, f1, support = precision_recall_fscore_support(y_test, y_pred_best, labels=classes)\n",
    "\n",
    "per_class_df = pd.DataFrame({\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': f1,\n",
    "    'Support': support\n",
    "}, index=classes)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.suptitle(f'Per-Class Performance - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Metrics by class\n",
    "per_class_df[['Precision', 'Recall', 'F1-Score']].plot(kind='bar', ax=axes[0], \n",
    "                                                         color=['steelblue', 'coral', 'lightgreen'])\n",
    "axes[0].set_title('Precision, Recall, F1 by Emotion', fontweight='bold')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_xlabel('Emotion')\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Support distribution\n",
    "per_class_df['Support'].plot(kind='bar', ax=axes[1], color='purple', edgecolor='black')\n",
    "axes[1].set_title('Test Samples per Emotion', fontweight='bold')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_xlabel('Emotion')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8accaacd",
   "metadata": {},
   "source": [
    "## 7. External Test Set Evaluation\n",
    "\n",
    "Now we evaluate our best model on the external test dataset from `Data Test/Emotions` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ff2d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data(test_path):\n",
    "    \"\"\"\n",
    "    Load external test data and extract features.\n",
    "    \"\"\"\n",
    "    X_test_ext, y_test_ext = [], []\n",
    "    test_files = glob.glob(os.path.join(test_path, \"**/*.wav\"), recursive=True)\n",
    "    print(f\"Found {len(test_files)} test audio files.\")\n",
    "    \n",
    "    for i, file_path in enumerate(test_files):\n",
    "        filename = os.path.basename(file_path)\n",
    "        \n",
    "        # Try to extract emotion from filename or folder\n",
    "        folder_name = os.path.basename(os.path.dirname(file_path)).lower()\n",
    "        \n",
    "        # Map folder names to emotions\n",
    "        emotion_label = None\n",
    "        if 'happy' in folder_name or 'happy' in filename.lower():\n",
    "            emotion_label = 'happy'\n",
    "        elif 'sad' in folder_name or 'sad' in filename.lower():\n",
    "            emotion_label = 'sad'\n",
    "        elif 'angry' in folder_name or 'angry' in filename.lower():\n",
    "            emotion_label = 'angry'\n",
    "        elif 'fear' in folder_name or 'fear' in filename.lower():\n",
    "            emotion_label = 'fearful'\n",
    "        elif 'disgust' in folder_name or 'disgust' in filename.lower():\n",
    "            emotion_label = 'disgust'\n",
    "        elif 'neutral' in folder_name or 'neutral' in filename.lower():\n",
    "            emotion_label = 'neutral'\n",
    "        elif 'calm' in folder_name or 'calm' in filename.lower():\n",
    "            emotion_label = 'calm'\n",
    "        elif 'supris' in folder_name or 'supris' in filename.lower():\n",
    "            emotion_label = 'surprised'\n",
    "        else:\n",
    "            # Try RAVDESS format\n",
    "            parts = filename.split('-')\n",
    "            if len(parts) == 7 and parts[2] in EMOTIONS:\n",
    "                emotion_label = EMOTIONS[parts[2]]\n",
    "        \n",
    "        if emotion_label is None:\n",
    "            continue\n",
    "        \n",
    "        # Extract features\n",
    "        features = extract_features(file_path)\n",
    "        if features is not None:\n",
    "            X_test_ext.append(features)\n",
    "            y_test_ext.append(emotion_label)\n",
    "        \n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"Processed {i + 1}/{len(test_files)} test files...\")\n",
    "    \n",
    "    return np.array(X_test_ext), np.array(y_test_ext)\n",
    "\n",
    "# Load external test data\n",
    "if os.path.exists('X_test_ext.npy') and os.path.exists('y_test_ext.npy'):\n",
    "    print(\"Loading cached external test data...\")\n",
    "    X_test_ext = np.load('X_test_ext.npy')\n",
    "    y_test_ext = np.load('y_test_ext.npy')\n",
    "else:\n",
    "    print(\"Extracting features from external test files...\")\n",
    "    X_test_ext, y_test_ext = load_test_data(TEST_DATA_PATH)\n",
    "    np.save('X_test_ext.npy', X_test_ext)\n",
    "    np.save('y_test_ext.npy', y_test_ext)\n",
    "\n",
    "print(f\"\\n✓ External test set loaded!\")\n",
    "print(f\"Feature matrix shape: {X_test_ext.shape}\")\n",
    "print(f\"Labels shape: {y_test_ext.shape}\")\n",
    "\n",
    "# Distribution\n",
    "test_emotion_counts = pd.Series(y_test_ext).value_counts().sort_index()\n",
    "print(f\"\\nExternal Test Set Distribution:\")\n",
    "print(test_emotion_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6e0100",
   "metadata": {},
   "source": [
    "### Evaluate Best Model on External Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7d7d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale external test data\n",
    "X_test_ext_scaled = scaler.transform(X_test_ext)\n",
    "\n",
    "# Apply polynomial transformation if needed\n",
    "if best_model_name == 'PolynomialLR':\n",
    "    X_test_ext_final = poly.transform(X_test_ext_scaled)\n",
    "    best_model = lr_poly\n",
    "elif best_model_name == 'SVM':\n",
    "    X_test_ext_final = X_test_ext_scaled\n",
    "    best_model = svm\n",
    "elif best_model_name == 'KNN':\n",
    "    X_test_ext_final = X_test_ext_scaled\n",
    "    best_model = knn\n",
    "else:\n",
    "    X_test_ext_final = X_test_ext_scaled\n",
    "    best_model = lr\n",
    "\n",
    "# Predict\n",
    "y_pred_ext = best_model.predict(X_test_ext_final)\n",
    "\n",
    "# Metrics\n",
    "ext_accuracy = accuracy_score(y_test_ext, y_pred_ext)\n",
    "ext_f1 = f1_score(y_test_ext, y_pred_ext, average='weighted')\n",
    "ext_precision = precision_score(y_test_ext, y_pred_ext, average='weighted')\n",
    "ext_recall = recall_score(y_test_ext, y_pred_ext, average='weighted')\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"EXTERNAL TEST SET RESULTS - {best_model_name}\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Accuracy:  {ext_accuracy:.4f}\")\n",
    "print(f\"F1-Score:  {ext_f1:.4f}\")\n",
    "print(f\"Precision: {ext_precision:.4f}\")\n",
    "print(f\"Recall:    {ext_recall:.4f}\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Comparison with internal test set\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Internal Test': [results[best_model_name]['accuracy'], results[best_model_name]['f1'], \n",
    "                      results[best_model_name]['precision'], results[best_model_name]['recall']],\n",
    "    'External Test': [ext_accuracy, ext_f1, ext_precision, ext_recall]\n",
    "}, index=['Accuracy', 'F1-Score', 'Precision', 'Recall'])\n",
    "\n",
    "print(\"\\nInternal vs External Test Performance:\")\n",
    "print(comparison_df)\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.suptitle(f'Internal vs External Test Performance - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "\n",
    "comparison_df.plot(kind='bar', ax=axes[0], color=['steelblue', 'coral'], edgecolor='black')\n",
    "axes[0].set_title('Metric Comparison', fontweight='bold')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].legend(['Internal Test', 'External Test'])\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Confusion matrix for external test\n",
    "cm_ext = confusion_matrix(y_test_ext, y_pred_ext, labels=best_model.classes)\n",
    "sns.heatmap(cm_ext, annot=True, fmt='d', cmap='Oranges', \n",
    "            xticklabels=best_model.classes, yticklabels=best_model.classes, \n",
    "            ax=axes[1], cbar_kws={'label': 'Count'})\n",
    "axes[1].set_title('External Test Confusion Matrix', fontweight='bold')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('True')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nExternal Test Set - Classification Report:\")\n",
    "print(classification_report(y_test_ext, y_pred_ext, target_names=best_model.classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032570b1",
   "metadata": {},
   "source": [
    "## 8. Model Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcd40c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model artifact\n",
    "model_artifact = {\n",
    "    'model': best_model,\n",
    "    'poly_generator': poly if best_model_name == 'PolynomialLR' else None,\n",
    "    'model_name': best_model_name\n",
    "}\n",
    "\n",
    "with open('model_parameters.pkl', 'wb') as f:\n",
    "    pickle.dump(model_artifact, f)\n",
    "\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(f\"✓ Best model ({best_model_name}) saved to 'model_parameters.pkl'\")\n",
    "print(f\"✓ Scaler saved to 'scaler.pkl'\")\n",
    "print(f\"\\nFinal Performance:\")\n",
    "print(f\"  Internal Test Accuracy: {results[best_model_name]['accuracy']:.4f}\")\n",
    "print(f\"  External Test Accuracy: {ext_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2389c9b",
   "metadata": {},
   "source": [
    "## 9. Conclusion & Key Insights\n",
    "\n",
    "### Summary\n",
    "- **Best Model:** Polynomial Logistic Regression with ~88% accuracy\n",
    "- **Generalization:** Model performs well on external test data\n",
    "- **Feature Engineering:** 112 handcrafted features effectively capture emotion patterns\n",
    "- **Implementation:** All models built from scratch without sklearn classifiers\n",
    "\n",
    "### Key Findings\n",
    "1. Polynomial features significantly improve performance\n",
    "2. MFCC features are most discriminative for emotion recognition\n",
    "3. Model generalizes well to unseen data from different sources\n",
    "\n",
    "### Future Work\n",
    "- Deep learning approaches (CNN, LSTM)\n",
    "- Data augmentation techniques\n",
    "- Real-time optimization\n",
    "- Multi-modal emotion recognition (audio + video)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
