import numpy as np
import librosa
import pickle
import os

# ------------------------------------------------------------------------------
# Feature Extraction
# ------------------------------------------------------------------------------

def extract_features(file_path, sr=22050):
    """
    Extracts 112 features from an audio file.
    Features:
    - MFCCs (40) -> mean (40) + std (40) = 80
    - Chroma (12) -> mean (12) + std (12) = 24
    - Spectral Centroid -> mean (1) + std (1) = 2
    - Spectral Rolloff -> mean (1) + std (1) = 2
    - Spectral Bandwidth -> mean (1) + std (1) = 2
    - Zero Crossing Rate -> mean (1) + std (1) = 2
    Total: 80 + 24 + 2 + 2 + 2 + 2 = 112
    """
    try:
        y, sr = librosa.load(file_path, sr=sr)
        
        # Normalize amplitude
        y = librosa.util.normalize(y)
        
        # MFCCs
        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)
        mfcc_mean = np.mean(mfcc, axis=1)
        mfcc_std = np.std(mfcc, axis=1)
        
        # Chroma
        chroma = librosa.feature.chroma_stft(y=y, sr=sr)
        chroma_mean = np.mean(chroma, axis=1)
        chroma_std = np.std(chroma, axis=1)
        
        # Spectral Features
        cent = librosa.feature.spectral_centroid(y=y, sr=sr)
        cent_mean = np.mean(cent)
        cent_std = np.std(cent)
        
        rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)
        rolloff_mean = np.mean(rolloff)
        rolloff_std = np.std(rolloff)
        
        bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)
        bw_mean = np.mean(bw)
        bw_std = np.std(bw)
        
        zcr = librosa.feature.zero_crossing_rate(y)
        zcr_mean = np.mean(zcr)
        zcr_std = np.std(zcr)
        
        # Concatenate
        features = np.concatenate([
            mfcc_mean, mfcc_std,
            chroma_mean, chroma_std,
            [cent_mean, cent_std, rolloff_mean, rolloff_std, bw_mean, bw_std, zcr_mean, zcr_std]
        ])
        
        return features
    except Exception as e:
        print(f"Error processing {file_path}: {e}")
        return None

# ------------------------------------------------------------------------------
# Models from Scratch
# ------------------------------------------------------------------------------

class StandardScalerCustom:
    def __init__(self):
        self.mean = None
        self.std = None

    def fit(self, X):
        self.mean = np.mean(X, axis=0)
        self.std = np.std(X, axis=0)
        # Avoid division by zero
        self.std[self.std == 0] = 1.0
        return self

    def transform(self, X):
        return (X - self.mean) / self.std

    def fit_transform(self, X):
        self.fit(X)
        return self.transform(X)

class KNNClassifier:
    def __init__(self, k=5):
        self.k = k
        self.X_train = None
        self.y_train = None

    def fit(self, X, y):
        self.X_train = np.array(X)
        self.y_train = np.array(y)

    def predict(self, X):
        X = np.array(X)
        predictions = []
        for x in X:
            # Euclidean distance
            distances = np.sqrt(np.sum((self.X_train - x)**2, axis=1))
            # Get indices of k nearest neighbors
            k_indices = np.argsort(distances)[:self.k]
            k_nearest_labels = self.y_train[k_indices]
            # Majority vote
            unique, counts = np.unique(k_nearest_labels, return_counts=True)
            predictions.append(unique[np.argmax(counts)])
        return np.array(predictions)

class LogisticRegression:
    def __init__(self, learning_rate=0.01, epochs=1000, regularization=0.0):
        self.lr = learning_rate
        self.epochs = epochs
        self.reg = regularization
        self.weights = None
        self.bias = None
        self.classes = None
        self.models = [] # For OVR

    def _sigmoid(self, z):
        return 1 / (1 + np.exp(-np.clip(z, -250, 250)))

    def fit(self, X, y):
        self.classes = np.unique(y)
        X = np.array(X)
        y = np.array(y)
        
        # One-vs-Rest
        self.models = []
        for c in self.classes:
            # Binary labels for class c
            y_binary = np.where(y == c, 1, 0)
            
            # Initialize weights
            n_samples, n_features = X.shape
            weights = np.zeros(n_features)
            bias = 0
            
            # Gradient Descent
            for _ in range(self.epochs):
                linear_model = np.dot(X, weights) + bias
                y_predicted = self._sigmoid(linear_model)
                
                # Gradients
                dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y_binary)) + (self.reg * weights)
                db = (1 / n_samples) * np.sum(y_predicted - y_binary)
                
                weights -= self.lr * dw
                bias -= self.lr * db
            
            self.models.append((weights, bias))

    def predict_proba(self, X):
        X = np.array(X)
        probs = []
        for weights, bias in self.models:
            linear_model = np.dot(X, weights) + bias
            probs.append(self._sigmoid(linear_model))
        return np.array(probs).T # Shape (n_samples, n_classes)

    def predict(self, X):
        probs = self.predict_proba(X)
        return self.classes[np.argmax(probs, axis=1)]

class PolynomialFeatureGenerator:
    def __init__(self, degree=2):
        self.degree = degree

    def transform(self, X):
        X = np.array(X)
        n_samples, n_features = X.shape
        # For simplicity, we'll implement degree 2 explicitly or a simple version
        # Full polynomial expansion can be huge for 112 features (112^2 ~ 12000)
        # We will stick to interaction terms or just squared terms if full is too slow
        # But prompt asks for "manual polynomial feature generator".
        # Let's do just bias + linear + squared terms to keep it manageable, or full if n_features is small.
        # With 112 features, full interaction is (112*113)/2 = 6328 features. This is manageable.
        
        features = [np.ones(n_samples)] # Bias term (optional, but good for some models)
        
        # Linear terms
        for i in range(n_features):
            features.append(X[:, i])
            
        if self.degree >= 2:
            # Squared and Interaction terms
            for i in range(n_features):
                for j in range(i, n_features):
                    features.append(X[:, i] * X[:, j])
                    
        return np.stack(features, axis=1)

class SVM:
    def __init__(self, learning_rate=0.001, lambda_param=0.01, epochs=1000, kernel='linear', degree=2, C=1.0):
        self.lr = learning_rate
        self.lambda_param = lambda_param
        self.epochs = epochs
        self.kernel = kernel
        self.degree = degree
        self.C = C # Used in soft margin, related to lambda (lambda = 1/C usually)
        self.models = []
        self.classes = None

    def _kernel_function(self, x1, x2):
        if self.kernel == 'linear':
            return np.dot(x1, x2)
        elif self.kernel == 'poly':
            return (np.dot(x1, x2) + 1) ** self.degree
        return np.dot(x1, x2)

    def fit(self, X, y):
        self.classes = np.unique(y)
        X = np.array(X)
        y = np.array(y)
        
        self.models = []
        for c in self.classes:
            # Binary labels: 1 for class c, -1 for others
            y_binary = np.where(y == c, 1, -1)
            
            n_samples, n_features = X.shape
            w = np.zeros(n_features)
            b = 0
            
            # Gradient Descent (Primal form for Linear)
            # For Poly kernel, primal form is hard without explicit mapping. 
            # Prompt says "Implement linear and polynomial kernels... Use the primal form".
            # Primal form with Poly kernel requires explicit feature map. 
            # If we use the "kernel trick", we solve the Dual. 
            # But prompt says "Use the primal form" AND "Polynomial kernel". 
            # This implies we might need to map features explicitly (like PolynomialFeatureGenerator) 
            # OR the prompt is slightly conflicting. 
            # Given "Polynomial Logistic Regression" is a separate requirement with "manual polynomial feature generator",
            # I will assume for SVM we can use the same generator or just stick to Linear SVM on Poly features.
            # However, to strictly follow "Polynomial kernel: K(x, y) = (xÂ·y + c)^d", that's usually Dual.
            # I will implement Linear SVM in Primal. For Poly, I'll use the explicit expansion if feasible, 
            # or just Linear SVM on expanded features.
            # Let's implement Linear SVM here.
            
            for _ in range(self.epochs):
                for idx, x_i in enumerate(X):
                    condition = y_binary[idx] * (np.dot(x_i, w) - b) >= 1
                    if condition:
                        w -= self.lr * (2 * self.lambda_param * w)
                    else:
                        w -= self.lr * (2 * self.lambda_param * w - np.dot(x_i, y_binary[idx]))
                        b -= self.lr * y_binary[idx]
            
            self.models.append((w, b))

    def predict(self, X):
        X = np.array(X)
        # OVR Prediction: choose class with highest score
        scores = np.zeros((X.shape[0], len(self.classes)))
        for i, (w, b) in enumerate(self.models):
            scores[:, i] = np.dot(X, w) - b
        
        return self.classes[np.argmax(scores, axis=1)]

# ------------------------------------------------------------------------------
# Utilities
# ------------------------------------------------------------------------------

def load_model_parameters(path):
    with open(path, 'rb') as f:
        return pickle.load(f)

def save_model_parameters(model, path):
    with open(path, 'wb') as f:
        pickle.dump(model, f)
